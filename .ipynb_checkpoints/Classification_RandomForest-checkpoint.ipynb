{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, log_loss\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_loaded = pd.read_csv(\"data2classificy_j01ca_n.csv\",sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop columns that must not be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Nome_Micro', 'IDADE', 'Genero', 'Latitude', 'Longitude',\n",
       "       'Target_J01CA', 'Diagnosticos_new', 'Nome_Analises_new',\n",
       "       'Produto_Analises_new', 'Sintomas_new', 'nr_past_visits',\n",
       "       'nr_past_visits_7days', 'nr_past_visits_15days', 'nr_past_infections',\n",
       "       'nr_past_infections_7days', 'nr_past_infections_15days', 'fever_7days',\n",
       "       'fever_15days', 'cough_7days', 'cough_15days', 'diarrhea_7days',\n",
       "       'diarrhea_15days', 'fatigue_7days', 'fatigue_15days',\n",
       "       'J01CA_past_resistance', 'J01GB_past_resistance',\n",
       "       'J01DH_past_resistance', 'J01MA_past_resistance',\n",
       "       'J01DD_past_resistance', 'Hour', 'Weekday', 'Monthday', 'Month', 'Year',\n",
       "       'DT_Admin_temp', 'Temperature_3avg', 'Temperature_3std',\n",
       "       'Temperature_3min', 'Humidity_3avg', 'Humidity_3std', 'Humidity_3min',\n",
       "       'Temperature_5avg', 'Temperature_5std', 'Temperature_5min',\n",
       "       'Humidity_5avg', 'Humidity_5std', 'Humidity_5min', 'Temperature_7avg',\n",
       "       'Temperature_7std', 'Temperature_7min', 'Humidity_7avg',\n",
       "       'Humidity_7std', 'Humidity_7min'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns2drop = ['ID_Isol','ID_Episodio','ID_Pedido','ID_Doente',\n",
    "                'Resistant_concat_Fam','Resistant_Anti','Infeccoes',\n",
    "                'DT_Admin','COLHEITA_DIFF','Dt_Colheita']\n",
    "data = data_loaded.drop(columns2drop,axis=1)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Nome_Micro                  4\n",
       "Latitude                  721\n",
       "Longitude                 721\n",
       "Diagnosticos_new        12674\n",
       "Nome_Analises_new        3743\n",
       "Produto_Analises_new     3743\n",
       "Sintomas_new            17352\n",
       "Temperature_3avg           23\n",
       "Temperature_3std           23\n",
       "Temperature_3min           23\n",
       "Humidity_3avg              23\n",
       "Humidity_3std              23\n",
       "Humidity_3min              23\n",
       "Temperature_5avg           45\n",
       "Temperature_5std           45\n",
       "Temperature_5min           45\n",
       "Humidity_5avg              45\n",
       "Humidity_5std              45\n",
       "Humidity_5min              45\n",
       "Temperature_7avg           56\n",
       "Temperature_7std           56\n",
       "Temperature_7min           56\n",
       "Humidity_7avg              56\n",
       "Humidity_7std              56\n",
       "Humidity_7min              56\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()[data.isnull().sum()>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see that are a bunch of variables that seem to have quite a amount of missing data. Looking to these variables we can divide them in 3 groups:\n",
    "* Resistant_concat_Fam, Resistant_Anti, Diagnosticos_new, Nome_Analises_new, Produto_Analises_new, Sintomas_new, Infeccoes: When these variables are nan in fact means that missing is not random it means that no information was placed. The solution is just to replace these values by a value like 'Nenhum' to tell that no values were annotated by the clinical staff.\n",
    "* Latitude/Longitude: No information was provided to these features solution (replace by a specific value and creat another feature indicating if these variable is missing)\n",
    "* Weather features: These values are missing because we do not have data for the year 2013."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['Diagnosticos_new'].fillna(\"Nenhum\",inplace=True)\n",
    "data['Produto_Analises_new'].fillna(\"Nenhum\",inplace=True)\n",
    "data['Nome_Analises_new'].fillna(\"Nenhum\",inplace=True)\n",
    "data['Sintomas_new'].fillna(\"Nenhum\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fill the nan values with the localization of one of the clinicals\n",
    "data['Longitude'].fillna(-9.1628837,inplace=True) \n",
    "data['Latitude'].fillna(38.748496,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['Nome_Micro'].fillna(\"Nenhum\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select variables of wather:\n",
    "weather_features = data.filter(regex='Temperature|Humidity').columns\n",
    "# Input values based on month and monthday\n",
    "for feat in weather_features:\n",
    "    data[feat] = data_loaded.groupby(['Month', 'Monthday'])[feat].transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()[data.isnull().sum()>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dumerize_bagofwords(dataset,feature,separator='--',drop_old=True):\n",
    "    dummies = dataset[feature].str.get_dummies(sep=separator)\n",
    "    for col_dummie_i in dummies.columns:\n",
    "        dummies = dummies.rename(columns = {col_dummie_i:str(feature)+ '_' + str(col_dummie_i)})\n",
    "        dummies = dummies.astype('uint8')\n",
    "    data = pd.concat([dataset,dummies],axis=1)\n",
    "    data = data.drop(feature,axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dummerize_categorical(dataset,feature,drop_old=True):\n",
    "    dummies = pd.get_dummies(dataset[feature],drop_first=True,prefix=feature)\n",
    "    dummies = dummies.astype('uint8')\n",
    "    data = pd.concat([dataset,dummies],axis=1)\n",
    "    data = data.drop(feature,axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_badfeatues(dataset,unique_thr=100):\n",
    "    print(\" ... Removing dummies that appear less than {} times:\".format(unique_thr),end=\" \")\n",
    "    # Copy dataset to avoid probels\n",
    "    dataset_out = dataset.copy()\n",
    "    # Select categorical variables\n",
    "    dataset_categorical = dataset_out.select_dtypes(include=['uint8'])\n",
    "    # List to store the names of features to keep\n",
    "    removidas = []\n",
    "    # Ltst to store the names of features to drop\n",
    "    mantidas = []\n",
    "    # Iterate thourgh the uint columns\n",
    "    for col in dataset_categorical.columns:\n",
    "        if dataset_categorical[col].sum() < unique_thr:\n",
    "            removidas.append(col)\n",
    "            dataset_out.drop([col],axis=1,inplace=True)\n",
    "        else:\n",
    "            mantidas.append(col)\n",
    "    # Print the number of columns to removed\n",
    "    print(\"{} removed.\".format(len(removidas)))\n",
    "    return dataset_out,removidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ... Removing dummies that appear less than 5 times: 0 removed.\n",
      " ... Removing dummies that appear less than 5 times: 0 removed.\n",
      " ... Removing dummies that appear less than 5 times: 0 removed.\n",
      " ... Removing dummies that appear less than 5 times: 0 removed.\n",
      " ... Removing dummies that appear less than 5 times: 0 removed.\n",
      " ... Removing dummies that appear less than 5 times: 0 removed.\n",
      " ... Removing dummies that appear less than 5 times: 310 removed.\n",
      " ... Removing dummies that appear less than 5 times: 0 removed.\n",
      " ... Removing dummies that appear less than 5 times: 0 removed.\n",
      " ... Removing dummies that appear less than 5 times: 0 removed.\n",
      " ... Removing dummies that appear less than 5 times: 0 removed.\n",
      " ... Removing dummies that appear less than 5 times: 39 removed.\n",
      " ... Removing dummies that appear less than 5 times: 0 removed.\n",
      " ... Removing dummies that appear less than 5 times: 1 removed.\n",
      " ... Removing dummies that appear less than 5 times: 0 removed.\n",
      " ... Removing dummies that appear less than 5 times: 0 removed.\n",
      " ... Removing dummies that appear less than 5 times: 0 removed.\n",
      " ... Removing dummies that appear less than 5 times: 80 removed.\n",
      " ... Removing dummies that appear less than 5 times: 0 removed.\n",
      " ... Removing dummies that appear less than 5 times: 361 removed.\n",
      " ... Removing dummies that appear less than 5 times: 0 removed.\n",
      " ... Removing dummies that appear less than 5 times: 0 removed.\n",
      " ... Removing dummies that appear less than 5 times: 110 removed.\n",
      " ... Removing dummies that appear less than 5 times: 27 removed.\n",
      " ... Removing dummies that appear less than 5 times: 1 removed.\n",
      " ... Removing dummies that appear less than 5 times: 0 removed.\n"
     ]
    }
   ],
   "source": [
    "numerical_features = data.filter(regex='nr_|IDADE|Latitude|Longitude|Humidity|Temperature').columns.tolist()\n",
    "categorical_features = list(set(data.columns.tolist()) - set(numerical_features))\n",
    "categorical_features\n",
    "\n",
    "data_new = data.copy()\n",
    "\n",
    "for feat_cat in categorical_features:\n",
    "    if \n",
    "        if data_new[feat_cat].astype('object').str.contains('--').any():\n",
    "            data_new = dumerize_bagofwords(data_new,feat_cat,separator='--',drop_old=True)\n",
    "        else:\n",
    "            data_new = dummerize_categorical(data_new,feat_cat,drop_old=True)       \n",
    "        data_new,removidas = remove_badfeatues(data_new,unique_thr=5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['fever_7days'].astype('str').str.contains('--').any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20108, 2291)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Weekday'].astype('object').str.contains('--').any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Transform cyclic variable\n",
    "#http://blog.davidkaleko.com/feature-engineering-cyclical-features.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model -  Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(njobs=-1,max_features='sqrt',n_estimators=10,oob_score=False)\n",
    "optimize_parameters = True\n",
    "\n",
    "if optimize_parameters:\n",
    "    # The scorers can be either be one of the predefined metric strings or a scorer\n",
    "    # callable, like the one returned by make_scorer\n",
    "    scoring = {'AUC': 'roc_auc', 'Accuracy': make_scorer(accuracy_score)}\n",
    "\n",
    "    param_grid = {\n",
    "        'n_estimators':[1,2,5,10,20,50,100,200,500],\n",
    "        'max_features':['auto','sqrt','log2',0.2,0.5,0.8]\n",
    "    }\n",
    "    CV_rfc = GridSearchCV(estimator=rfc,param_grid=param_grid,cv=5,scoring='roc_curve')\n",
    "else:\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_roc_cnf(y_true,y_score,y_predicted):\n",
    "    fpr,tpr,thresholds = roc_curve(y_true,y_score[:,1])\n",
    "    roc_auc = auc(fpr,tpr)\n",
    "    \n",
    "    plt.figure(figsize=(10,5))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(fpr,tpr,color='darkorange',lw=2,label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0,1],[0,1],color='navy',lw=lw,linestyle='--')\n",
    "    plt.xlim([0.0,1.0])\n",
    "    plt.ylim([0.0,1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.xlabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc='lower right')\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    mat = confusion_matrix(y_true,y_predicted)\n",
    "    sns.heatmap(mat.T, square = True, annot=True, fmt='d', char=False, cmap = 'Blues')\n",
    "    plt.xlabel('true label')\n",
    "    plt.ylabel('predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
